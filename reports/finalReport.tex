\documentclass[10pt,a4paper]{paper}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}

\author{Elise Kuylen \and Robin Verachtert}
\title{Information Retrieval Project}
\begin{document}
\maketitle

\section{Introduction}

For this project, we implemented a \textit{content-based} recommendation system for books. Based on which books a user has liked before, we tried to recommend new books, that were similar in content and would (hopefully) also interest the user.

\section{Algorithms implemented}

To create our recommendation system, we first had to index a set of documents - the summaries and reviews of the books in our database. Next, we constructed tf-idf vectors for these books, which we could then compare to determine the similarity of different books. We also implemented dimensionality reduction. To perform a dimensionality reduction we had to use Singular Value Decomposition. We consulted the chapter in the book describing the dimensionality reduction.  

\subsection{Indexing a collection of documents}

\subsection{Vector space model}

\subsection{Dimensionality reduction}


\section{Design choices}

\subsection{Dataset collection}

To get a sufficiently large dataset of books and user reviews of those books, we scraped the Goodreads database. Goodreads is a social platform that allows users to rate and review the books they have read. \\
We saved the information we got from Goodreads in JSON format, since this is easy to parse.
We collected some different datasets, of different sizes, so we had a good variety to test the project on. There is a small dataset, only containing books of 2 users, this is the smallest set, which we used to quickly test newly implemented parts. Then there is a bookset of about 2000 books and one of about 10000 books. The last one we mostly used to test the speed of the indexer, because on our normal computers the similarity computation took too long.

We also have an additional dataset containing reviews of clothing, shoes,and jewelry from Amazon \footnote{\url{http://jmcauley.ucsd.edu/data/amazon/}}. This is a larger dataset containing 250000 reviews. This dataset is exclusively used for testing the speed of the indexer.

\subsection{Lucene for indexing}

To index the summaries and reviews of the books that we had scraped from Goodreads, we used the Lucene library. It has a lot of already implemented methods, making so that we could focus on the recommender part and most importantly the evaluation. For indexing we used a English Analyzer to pre-process the data and perform stemming.


\subsection{Creating tf-idf vectors}

To store the TF-IDF vectors of our books, we create the class TFIDFBookVector.
%TODO sparse vector
%TODO calculation of weights

%TODO matrix

\subsection{Recommendation}

To make recommendations we used two different strategies:
\begin{itemize}
\item \textbf{Based on one book:} To make recommendations based on one book in the user's profile, we treated the text of the summary and the reviews for this book as a query. Next, we constructed a TF-IDF vector for this query, and compared it to the TF-IDF vectors in our matrix, using the cosine similarity measure. Once this was done, we sorted the books according to their similarity with the TF-IDF vector for the user's book and returned the first 10 results.
\item \textbf{Based on all books in the user's profile} %TODO different strategies: comparing top 20s / adding vectors / adding cosine similarities
\end{itemize}

\subsection{Dimensionality reduction}

We decided to not implement this ourselves, because there are enough libraries supporting it.\\
The SVD library we used is UJMP \footnote{\url{https://ujmp.org/}}. This also provided a sparse matrix implementation which is relevant to the total memory consumed. The implementation is very straightforward. You start with a matrix M x N and perform the SVD which provides 3 matrixes. $U$ (M x M), $\Sigma$ (M x N) and $V$ (N x N). Then you reduce the complexity by keeping the k first columns of $U$ and $V$ and making $\Sigma$ a k x k matrix with the k highest singular values on the diagonal. From this you can compute the term matrix again (in the reduced space) by doing:
\[
M_k = U_k * \Sigma_k * V_k^T
\]
When we want to match a query we have to also map this query vector to the reduced dimension space.
\[
q_k = \Sigma_k^{-1} * U_k^t * q
\]
This gives a vector of the same size as $q$, so we can use it the same as before to query against the term Matrix.

\section{Evaluation results}

\subsection{Indexing}

To evaluate the quality of our indexing process, we timed how long it took to run the indexer for different sizes of datasets. We checked the time consumption for regular indexing as well as for indexing followed by dimensionality reduction.

\begin{tabular} {l|l}
Dataset & Time Consumption \\ \hline
Clothing,Shoes,and Jewelry data set from Amazon (150MB) \footnote{\url{http://jmcauley.ucsd.edu/data/amazon/}} &  45388 ms \\ \hline
10000 books dataset & 9332 ms \\ \hline

180 books dataset & 2800 ms \\ \hline

Note that these were times gotten on a low performance computer (less than 4GB RAM) so when running on a faster system these times would probably improve.

\end{tabular}
\subsubsection{Regular}

%TODO fancy table with times for different sizes of dataset

\subsubsection{With dimensionality reduction}

%TODO fancy table with times for different sizes of dataset

\subsection{Recommendation}

To evaluate our recommendation process, we looked at the precision of our results.\footnote{Recall is less relevant here, and it is also nigh impossible to check: we would have to decide manually, for each book in our dataset, whether a book is relevant for a particular user or not.}

%TODO data books_reviews.json

\subsubsection{Regular}

\paragraph{Recommendation based on single book}
To evaluate the results of recommendation based on one book, we let the system make a top-10 of recommendations for each book for which our test user had given a score of 4 stars or higher. We then compared the books in this top-10 with the other books the user had liked previously, and calculated the precision @ 10.
The average precision @ 10 for user XXX with data XXX was XXX. %TODO fill in which data set etc we use to test

\paragraph{Recommendation based on multiple books}
To evaluate recommendation based on a set of books, we used 80\% of the user's 'likes' (books that were given 4 stars or more) as a training set to base our recommendations on. We used the 3 techniques described above to generate a top-10 of recommendations for our test user. Then we compared the content of this top-10 with the remaining 20\% of the user's 'likes', which we used as our test set.
From this, we again calculated precision @ 10.
This gave us the following results:
\begin{itemize}
\item \textbf{Comparing top-10s of separate books:} %TODO result with correct user and data set
\item \textbf{Adding vectors of books:}
\item \textbf{Adding cosine similarity results:}
\end{itemize}

\subsubsection{With dimensionality reduction}
Because SVD is a very costly operation, we only used a very small data set to test this on. Otherwise we got a Heap Memory Overflow.
%TODO results of precision @ 10 for different users
% graph?
% list?

\section{Code}
The code created in this project can be found at \url{https://github.com/verachtertr/IR_project}
\end{document}