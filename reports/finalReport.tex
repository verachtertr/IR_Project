\documentclass[10pt,a4paper]{paper}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\author{Elise Kuylen \and Robin Verachtert}
\title{Information Retrieval Project}
\begin{document}
\maketitle

\section{Introduction}

For this project, we implemented a \textit{content-based} recommendation system for books. Based on which books a user has like before, we tried to recommend new books, that were similar in content and would (hopefully) also interest the user.

\section{Algorithms implemented}

To create our recommendation system, we first had to index a set of documents - the summaries and reviews of the books in our database. Next, we constructed tf-idf vectors for these books, which we could then compare to determine the similarity of different books. We also implemented dimensionality reduction. %TODO some more about dimensionality reduction

\subsection{Indexing a collection of documents}

\subsection{Vector space model}

\subsection{Dimensionality reduction}

\section{Design choices}

\subsection{Dataset collection}

To get a sufficiently large dataset of books and user reviews of those books, we scraped the Goodreads database. Goodreads is a social platform that allows users to rate and review the books they have read. \\
We saved the information we got from Goodreads in JSON format, since this is easy to parse.

\subsection{Lucene for indexing}

To index the summaries and reviews of the books that we had scraped from Goodreads, we used the Lucene library. %TODO some more about how we exactly indexed, English analyser for tokenization, ...

\subsection{Creating tf-idf vectors}

To store the TF-IDF vectors of our books, we create the class TFIDFBookVector.
%TODO sparse vector
%TODO calculation of weights

%TODO matrix

\subsection{Recommendation}

To make recommendations we used two different strategies:
\begin{itemize}
\item \textbf{Based on one book:} To make recommendations based on one book in the user's profile, we treated the text of the summary and the reviews for this book as a query. Next, we constructed a TF-IDF vector for this query, and compared it to the TF-IDF vectors in our matrix, using the cosine similarity measure. Once this was done, we sorted the books according to their similarity with the TF-IDF vector for the user's book and returned the first 10 results.
\item \textbf{Based on all books in the user's profile} %TODO different strategies: comparing top 20s / adding vectors / adding cosine similarities
\end{itemize}

\subsection{Dimensionality reduction}

%TODO explanation about how we did dimensionality reduction

\section{Evaluation results}

\subsection{Indexing}

To evaluate the quality of our indexing process, we timed how long it took for different sizes of datasets. We checked the time consumption for regular indexing as well as for indexing followed by dimensionality reduction. %TODO should we do space consumption?
\subsubsection{Regular}

%TODO fancy table with times for different sizes of dataset

\subsubsection{With dimensionality reduction}


%TODO fancy table with times for different sizes of dataset

\subsection{Recommendation}

To evaluate our recommendation process, we looked at the precision of our results.\footnote{Recall is less relevant here, and it is also nigh impossible to check: we would have to decide manually, for each book in our dataset, whether a book is relevant for a particular user or not.}

%TODO data books_reviews.json

\subsubsection{Regular}

\paragraph{Recommendation based on single book}
To evaluate the results of recommendation based on one book, we let the system make a top-10 of recommendations for each book for which our test user had given a score of 4 stars or higher. We then compared the books in this top-10 with the other books the user had liked previously, and calculated the precision @ 10.
The average precision @ 10 for user XXX with data XXX was XXX. %TODO fill in which data set etc we use to test

\paragraph{Recommendation based on multiple books}
To evaluate recommendation based on a set of books, we used 80\% of the user's 'likes' (books that were given 4 stars or more) as a training set to base our recommendations on. We used the 3 techniques described above to generate a top-10 of recommendations for our test user. Then we compared the content of this top-10 with the remaining 20\% of the user's 'likes', which we used as our test set.
From this, we again calculated precision @ 10.
This gave us the following results:
\begin{itemize}
\item \textbf{Comparing top-10s of separate books:} %TODO result with correct user and data set
\item \textbf{Adding vectors of books:}
\item \textbf{Adding cosine similarity results:}
\end{itemize}

\subsubsection{With dimensionality reduction}

%TODO results of precision @ 10 for different users
% graph?
% list?

\section{External sources}

\end{document}