\documentclass[10pt,a4paper]{paper}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\author{Elise Kuylen \and Robin Verachtert}
\title{Information Retrieval Project}
\begin{document}
\maketitle

\section{Introduction}

For this project, we implemented a \textit{content-based} recommendation system for books. Based on which books a user has like before, we tried to recommend new books, that were similar in content and would (hopefully) also interest the user.

\section{Algorithms implemented}

To create our recommendation system, we first had to index a set of documents - the summaries and reviews of the books in our database. Next, we constructed TF-IDF vectors for these books, which we could then compare to determine the similarity of different books. Comparing documents on the basis of their TF-IDF vectors was done using the cosine similarity measure.

We also implemented dimensionality reduction. %TODO some more about dimensionality reduction

\section{Design choices}

\subsection{Dataset collection}

To get a sufficiently large dataset of books and user reviews of those books, we scraped the Goodreads database. Goodreads is a social platform that allows users to rate and review the books they have read. \\
We saved the information we got from Goodreads in JSON format, since this is easy to parse.

\subsection{Lucene for indexing}

To index the summaries and reviews of the books that we had scraped from Goodreads, we used the Lucene library. %TODO some more about how we exactly indexed, English analyser for tokenization, ...

\subsection{Creating tf-idf vectors}

To store the TF-IDF vectors of our books, we create the class TFIDFBookVector. Since many of the terms in our document collection will not be present in the term vector of one document, we used a sparse vector to save the TF-IDF weights of these terms. Apart from that, we also saved some information about the book, such as title, ISBN and author.\\
TF-IDF weights were calculated from the information present in the index: the term vectors of the documents and the overall frequency of terms in the document collection.\\
We saved 


\subsection{Recommendation}

To make recommendations we used two different strategies:
\begin{itemize}
\item \textbf{Based on one book:} To make recommendations based on one book in the user's profile, we treated the text of the summary and the reviews for this book as a query. Next, we constructed a TF-IDF vector for this query, and compared it to the TF-IDF vectors in our matrix, using the cosine similarity measure. Once this was done, we sorted the books according to their similarity with the TF-IDF vector for the user's book and returned the first 10 results.
\item \textbf{Based on all books in the user's profile} While making recommendations on the basis of one book the user has liked is interesting, it would be more interesting if we could recommend books based on the entire set of books that a user has enjoyed. However, we could not discover any algorithms that used a Vector Space Model to do this. Thus, we experimented with three different techniques to try to find a solution to this problem:
\begin{itemize}
%TODO different strategies: comparing top 20s / adding vectors / adding cosine similarities
\end{itemize}
\end{itemize}

\subsection{Dimensionality reduction}

%TODO explanation about how we did dimensionality reduction

\section{Evaluation results}

\subsection{Indexing}

To evaluate the quality of our indexing process, we timed how long it took for different sizes of datasets. We checked the time consumption for regular indexing as well as for indexing followed by dimensionality reduction. %TODO should we do space consumption?
\subsubsection{Regular}

%TODO fancy table with times for different sizes of dataset

\subsubsection{With dimensionality reduction}


%TODO fancy table with times for different sizes of dataset

\subsection{Recommendation}

To evaluate our recommendation process, we looked at the precision of our results.\footnote{Recall is less relevant here, and it is also nigh impossible to check: we would have to decide manually, for each book in our dataset, whether a book is relevant for a particular user or not.} We used two users to test our system: a real user account, that of Robin Verachtert, and 'IR Test', a user account that we created for this project, and which mainly contains Fantasy novels.

\subsubsection{Regular}

\paragraph{Recommendation based on single book}
To evaluate the results of recommendation based on one book, we let the system make a top-10 of recommendations for each book for which our test user had given a score of 4 stars or higher. We then compared the books in this top-10 with the other books the user had liked previously, and calculated the precision @ 10.
The average precision @ 10 for user 'IR Test' with a small of books to recommend from was 0.567. 
The average precision @ 10 for user 'IR Test' with a larger set of books to recommend from was 0.247.

The average precision @ 10 for user 'Robin Verachtert' with a small of books to recommend from was 0.448. 
The average precision @ 10 for user 'Robin Verachtert' with a larger set of books to recommend from was 0.173.

We also checked the books that were recommend manually, to find an explanation for the rather low results for precision. We found that things like names of people and countries were very influential in deciding which books were recommended - since they are often rare in the complete set of documents that was indexed. However, names and countries often do not decide the genre or subject of a book, an thus are not very relevant for recommendation. 

\paragraph{Recommendation based on multiple books}
To evaluate recommendation based on a set of books, we used 80\% of the user's 'likes' (books that were given 4 stars or more) as a training set to base our recommendations on. We used the 3 techniques described above to generate a top-10 of recommendations for our test user. Then we compared the content of this top-10 with the remaining 20\% of the user's 'likes', which we used as our test set.
From this, we again calculated precision @ 10.
This gave us the following results:
\begin{itemize}
\item \textbf{Comparing top-10s of separate books:} %TODO result with correct user and data set
\item \textbf{Adding vectors of books:}
\item \textbf{Adding cosine similarity results:}
\end{itemize}

\subsubsection{With dimensionality reduction}

After adding dimensionality reduction, we ran the tests mentioned above, on a small dataset of books, and got the following results.

\paragraph{Recommendation based on single book}

The average precision @ 10 for user 'IR Test' with a small of books to recommend from was 0.411.
The average precision @ 10 for user 'Robin Verachtert' with a small of books to recommend from was 0.370.

\paragraph{Recommendation based on multiple books}

\begin{itemize}
\item \textbf{Comparing top-10s of separate books:} %TODO result with correct user and data set
\item \textbf{Adding vectors of books:}
\item \textbf{Adding cosine similarity results:}
\end{itemize}

\section{External sources}

\end{document}